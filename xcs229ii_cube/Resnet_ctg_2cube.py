# -*- coding: utf-8 -*-
"""data_loader_and_solution_length_approximator_2cube.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14qJnkw_wE9YGOoCAM-qOmO_kd6KBr2q7
"""
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pdb


# code block is to get the computation device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)

class NumpyDataset(Dataset):

    def __init__(self, input_file_name, output_file_name, is_train=True):
        X_in = pd.read_csv(input_file_name)
        X_in.drop(columns=['ID'], inplace=True)
        y_in = pd.read_csv(output_file_name)
        y_in.drop(columns=['ID'], inplace=True)
        X_in['distance'] = y_in['distance']
        self.len = len(X_in)

        from sklearn.model_selection import train_test_split

        X_train, X_test = train_test_split(X_in, test_size=.3, random_state=1)

        if is_train:
            X = X_train
        else:
            X = X_test

        self.class_holder = dict()
        for i in range(1, 15):
            subsetX = X[X['distance'] == i].copy()
            subsetX.drop(columns=['distance'], inplace=True)
            self.class_holder[i] = subsetX.to_numpy()

    def __getitem__(self, index):

        choice_idx = np.random.choice(range(1, 15))

        x = self.class_holder[choice_idx]
        while len(x) == 0:
            choice_idx = np.random.choice(range(1, 15))
            x = self.class_holder[choice_idx]

        x_choice = x[np.random.choice(range(len(x)))]

        return x_choice, choice_idx

    def __len__(self):
        return self.len

train_dataset = NumpyDataset('ctg/train_input.csv', 'ctg/train_output.csv')
trainloader = DataLoader(train_dataset, batch_size=64, shuffle=False)
valid_dataset = NumpyDataset('ctg/train_input.csv', 'ctg/train_output.csv', is_train=False)
testloader = DataLoader(valid_dataset, batch_size=64)

class ResnetModel(nn.Module):
    def __init__(self, state_dim: int, one_hot_depth: int, h1_dim: int, resnet_dim: int, num_resnet_blocks: int,
                 out_dim: int, batch_norm: bool):
        super().__init__()
        self.one_hot_depth: int = one_hot_depth
        self.state_dim: int = state_dim
        self.blocks = nn.ModuleList()
        self.num_resnet_blocks: int = num_resnet_blocks
        self.batch_norm = batch_norm

        # first two hidden layers
        if one_hot_depth > 0:
            self.fc1 = nn.Linear(self.state_dim * self.one_hot_depth, h1_dim)
        else:
            self.fc1 = nn.Linear(self.state_dim, h1_dim)

        if self.batch_norm:
            self.bn1 = nn.BatchNorm1d(h1_dim)

        self.fc2 = nn.Linear(h1_dim, resnet_dim)

        if self.batch_norm:
            self.bn2 = nn.BatchNorm1d(resnet_dim)

        # resnet blocks
        for block_num in range(self.num_resnet_blocks):
            if self.batch_norm:
                res_fc1 = nn.Linear(resnet_dim, resnet_dim)
                res_bn1 = nn.BatchNorm1d(resnet_dim)
                res_fc2 = nn.Linear(resnet_dim, resnet_dim)
                res_bn2 = nn.BatchNorm1d(resnet_dim)
                self.blocks.append(nn.ModuleList([res_fc1, res_bn1, res_fc2, res_bn2]))
            else:
                res_fc1 = nn.Linear(resnet_dim, resnet_dim)
                res_fc2 = nn.Linear(resnet_dim, resnet_dim)
                self.blocks.append(nn.ModuleList([res_fc1, res_fc2]))

        # output
        self.fc_out = nn.Linear(resnet_dim, out_dim)

    def forward(self, states_nnet):
        x = states_nnet

        # preprocess input
        if self.one_hot_depth > 0:
            x = F.one_hot(x.long(), self.one_hot_depth)
            x = x.float()
            x = x.view(-1, self.state_dim * self.one_hot_depth)
        else:
            x = x.float()

        # first two hidden layers
        x = self.fc1(x)
        if self.batch_norm:
            x = self.bn1(x)

        x = F.relu(x)
        x = self.fc2(x)
        if self.batch_norm:
            x = self.bn2(x)

        x = F.relu(x)

        # resnet blocks
        for block_num in range(self.num_resnet_blocks):
            res_inp = x
            if self.batch_norm:
                x = self.blocks[block_num][0](x)
                x = self.blocks[block_num][1](x)
                x = F.relu(x)
                x = self.blocks[block_num][2](x)
                x = self.blocks[block_num][3](x)
            else:
                x = self.blocks[block_num][0](x)
                x = F.relu(x)
                x = self.blocks[block_num][1](x)

            x = F.relu(x + res_inp)

        # output
        x = self.fc_out(x)
        return x


net = ResnetModel(24, 6, 1000, 100, 4, 1, True)
net = net.to(device)

# loss
criterion = nn.MSELoss()
# optimizer
optimizer = optim.Adam(net.parameters(), lr=0.0001)


def train(net, trainloader):
    i = 0
    for epoch in range(2):  # no. of epochs
        running_loss = 0
        for data in trainloader:
            print("Training")
            # data pixels and labels to GPU if available

            inputs, labels = data[0].to(device, non_blocking=True) - 1, data[1].to(device, non_blocking=True)

            #pdb.set_trace()


            # set the parameter gradients to zero
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels.view(-1).float())
            # propagate the loss backward
            loss.backward()
            # update the gradients
            optimizer.step()

            running_loss += loss.item()

            if not i % 500 and i > 0:
                print(running_loss / i)
            i += 1
        print('[Epoch %d] loss: %.3f' %
              (epoch + 1, running_loss / len(trainloader)))

    print('Done Training')


def test(net, testloader):
    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            inputs, labels = data[0].to(device, non_blocking=True) - 1, data[1].to(device, non_blocking=True)
            outputs = net(inputs)
            pdb.set_trace()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print('Accuracy of the network on test set: %0.3f %%' % (100 * correct / total))


train(net, trainloader)
# net.load_state_dict(torch.load('ctg_2cube_model3.pt'))
test(net, testloader)
#torch.save(net.state_dict(), 'ctg_2cube_model2.pt')



